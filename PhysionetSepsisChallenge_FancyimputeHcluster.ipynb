{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PhysionetSepsisChallenge_FancyimputeHcluster (1).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Yf7BEeCW9qm_","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","import scipy.stats as ss\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from __future__ import print_function\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.metrics.cluster import homogeneity_score\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import matplotlib.cm as cm\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKD43sty--IR","colab_type":"code","colab":{}},"source":["# ! pip install fancyimpute"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMa9RZXy_IHY","colab_type":"code","colab":{}},"source":["from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler, IterativeImputer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZ_6_ZdM_PQw","colab_type":"code","outputId":"ae896b40-206c-4af6-a3db-a9d37fc4b3e2","executionInfo":{"status":"ok","timestamp":1574301309764,"user_tz":-330,"elapsed":3100,"user":{"displayName":"Ajith K J","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDyUvY5gPUwfLZ3cBTFSPQYzisO_-0eh9ViTDx3Zw=s64","userId":"12087170633654812471"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive') "],"execution_count":9,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tb4pVD0mZc2b","colab_type":"code","colab":{}},"source":["# Read data and impute missing values\n","#os.chdir('/content/drive/My Drive/Colab Notebooks/PhysioNet_Sepsis_Challenge')\n","# os.chdir('/content/drive/My Drive/Colab Notebooks/Physionet_sepsis_project/Data/Data_partial')\n","# os.chdir('/content/drive/My Drive/Colab Notebooks/Physionet_sepsis_project/Data/Data_full')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/Physionet_sepsis_project/Data/Data_full')\n","# /content/drive/My Drive/Colab Notebooks/Demo_Physionet/Data\n","\n","i = 0\n","df = pd.DataFrame()\n","for file in glob.iglob('*.psv'):\n","    f_name = float(os.path.splitext(os.path.basename(file))[0][1:])  \n","    # print(f_name)  \n","    tempdf = pd.read_csv(file, sep = '|', index_col = None, header = 0)    \n","    tempdf['Hour'] = tempdf.index\n","    tempdf['Identifier'] = f_name\n","    df = pd.concat([df, tempdf], axis=0)\n","    i = i+1\n","\n","# Names of all columns in the data that contain physiological data\n","physiological_cols = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2',\n","       'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN',\n","       'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n","       'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n","       'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n","       'Fibrinogen', 'Platelets']\n","\n","# Names of all columns in the data that contain demographic data\n","demographic_cols = ['Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']\n","\n","# Columns of features #730\n","feature_cols = physiological_cols + ['Hour'] + demographic_cols + ['Identifier']\n","\n","# The name of the column that contains the value we are trying to predict\n","label_col = 'SepsisLabel'\n","\n","#cols = list(df)\n","cols = feature_cols + [label_col]\n","\n","# Move the SepsisLabel column to end of dataframe\n","#cols.insert(len(cols), cols.pop(cols.index('SepsisLabel')))\n","df = df.loc[:,cols]\n","# Plot percentage of missing values (NaNs) for each feature\n","cutoff = 60\n","fig = plt.figure(figsize=(20,10))\n","percent_missing = (df.isna().sum()/df.shape[0])*100\n","percent_missing.plot(kind=\"bar\")\n","plt.plot(percent_missing, np.array([cutoff for i in range(len(percent_missing))]), 'r--') \n","fig.suptitle('Percentage Missing Values', fontsize=20)\n","plt.xlabel('Feature', fontsize=16)\n","plt.ylabel('% Missing Values', fontsize=16)\n","plt.savefig('histogram.jpg')\n","\n","# Retain columns in dataframe with <= cutoff% missing values \n","df = df.loc[:, df.columns[percent_missing <= cutoff]]\n","print('Retained features:')\n","print(df.columns.values)\n","\n","feature_cols = df.columns.values[:-1]\n","# Adjust physiological and demographic column names\n","\n","physiological_cols = [x for x in feature_cols if x in set(physiological_cols)]\n","demographic_cols = [x for x in feature_cols if x in set(demographic_cols)]\n","\n","# Impute missing data using fancyimpute package \n","df_filled = pd.DataFrame(data = IterativeImputer().fit_transform(df[df.columns[:-1]].to_numpy()),\n","                         columns = df.columns[:-1],\n","                         index = df.index)\n","\n","\n","pd.set_option('display.expand_frame_repr', False)\n","df_filled['SepsisLabel'] = df['SepsisLabel']\n","print(df_filled.head())\n","print(\"files scanned\",i)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMwTaorQDOz3","colab_type":"code","colab":{}},"source":["continuous_cols = list(df_filled[df_filled.columns[0:6]].columns.values)\n","ordinal_cols = list(['Hour', 'Age','HospAdmTime','ICULOS'])\n","# binary_cols = list(['Gender', 'Unit1', 'Unit2'])\n","binary_cols = list(['Gender'])\n","# print(df_filled[\"Hour\"].head())\n","df_filled[ordinal_cols] = df_filled[ordinal_cols].round()\n","df_filled[binary_cols] = df_filled[binary_cols].round()\n","\n","sepsis = df_filled[\"SepsisLabel\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPiioVBBHPaj","colab_type":"code","colab":{}},"source":["# Calculate correlation between continuous & continuous variables\n","# using Pearson's Rho\n","print(continuous_cols)\n","print(ordinal_cols) \n","corr = pd.DataFrame(columns = continuous_cols + ordinal_cols + binary_cols,\n","                    index =  continuous_cols + ordinal_cols + binary_cols)\n","con_con_corr = df_filled[continuous_cols].corr(method ='pearson')\n","corr[continuous_cols] = con_con_corr\n","print(corr.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INKhKSv-JO7K","colab_type":"code","colab":{}},"source":["# Function to calculate correlation between (continuous & ordinal) and \n","# (continuous & binary) variables using Correlation Ratio\n","\n","def correlation_ratio(categories, measurements):\n","    fcat, _ = pd.factorize(categories)\n","    cat_num = np.max(fcat)+1\n","    y_avg_array = np.zeros(cat_num)\n","    n_array = np.zeros(cat_num)\n","    for i in range(0,cat_num):\n","        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n","        n_array[i] = len(cat_measures)\n","        y_avg_array[i] = np.average(cat_measures)\n","    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n","    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n","    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n","    if numerator == 0:\n","        eta = 0.0\n","    else:\n","        eta = np.sqrt(numerator/denominator)\n","    return eta"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOUqjzCgH-Ne","colab_type":"code","colab":{}},"source":["# Calculate correlation between (continuous & ordinal) and \n","# (continuous & binary) variables using Correlation Ratio\n","for con_col_name in continuous_cols:\n","\n","  for bin_col_name in binary_cols:\n","    val = correlation_ratio(df_filled[con_col_name].values,\n","                            df_filled[bin_col_name].values)\n","    \n","    corr.loc[con_col_name, bin_col_name] = val\n","    corr.loc[bin_col_name, con_col_name] = val\n","  for ord_col_name in ordinal_cols:\n","    val = correlation_ratio(df_filled[con_col_name].values,\n","                            df_filled[ord_col_name].values)\n","    \n","    corr.loc[con_col_name, ord_col_name] = val\n","    corr.loc[ord_col_name, con_col_name] = val\n","print(corr)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnEi6JslbHnr","colab_type":"code","colab":{}},"source":["# Function to calculate correlation between (ordinal & ordinal),\n","# (ordinal, binary) and (binary & binary) variables using\n","# Cramer's V \n","\n","def cramers_phi(x, y):\n","    confusion_matrix = pd.crosstab(x,y)\n","    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n","    n = confusion_matrix.sum().sum()\n","    phi2 = chi2/n\n","    r,k = confusion_matrix.shape\n","    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n","    rcorr = r-((r-1)**2)/(n-1)\n","    kcorr = k-((k-1)**2)/(n-1)\n","    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n","\n","# Plot the correlation matrix\n","#sns.heatmap(corr, annot = True) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJkPgbAHN3x0","colab_type":"code","colab":{}},"source":["# Calculate correlation between (ordinal & ordinal) and\n","# (binary & binary) variables using Cramer's V\n","for col in [ordinal_cols, binary_cols]:\n","  for i in range(0,len(col)):\n","    for j in range(i,len(col)):\n","      if i == j:                 \n","        corr.loc[col[i],col[j]] = 1.0            \n","      else:                 \n","        val = cramers_phi(df_filled[col[i]].values,\n","                          df_filled[col[j]].values) \n","        corr.loc[col[i], col[j]] = val\n","        corr.loc[col[j], col[i]] = val\n","\n","# Calculate correlation between (ordinal & binary)\n","# variables using Cramer's V\n","iteration = 0\n","for ord_col_name in ordinal_cols:\n","  for bin_col_name in binary_cols:\n","    val = cramers_phi(df_filled[ord_col_name].values,\n","                      df_filled[bin_col_name].values) \n","    corr.loc[ord_col_name, bin_col_name] = val\n","    corr.loc[bin_col_name, ord_col_name] = val\n","    iteration = iteration+1\n","\n","print(corr)\n","print(\"iterations\",iteration)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pog5baf7b3eY","colab_type":"code","colab":{}},"source":["\n","# generate the linkage matrix\n","Z = linkage(corr, 'ward')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIcjF8CkYk8n","colab_type":"code","colab":{}},"source":["# Calculate full dendrogram \n","plt.figure(figsize=(12, 12))\n","plt.title('Hierarchical Clustering Dendrogram')\n","plt.xlabel('Variables')\n","plt.ylabel('distance')\n","dendrogram(Z, labels=list(corr.columns))\n","plt.axhline(y=2, color='r', linestyle='--')\n","plt.show()\n","plt.savefig('dendogram.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FReIbNMRefCU","colab_type":"code","colab":{}},"source":["# Calculate cluster scores using PCA\n","pca = PCA(n_components = 1)\n","# Do the following for each cluster to get the cluster scores which will be\n","# the new features. \n","# cluster_data = df_filled[\"#give the column names corresponding to the cluster#\"].values\n","# cluster_data_1 = df_filled[[\"Hour\",\"ICULOS\"]].values\n","# cluster_data_2 = df_filled[[\"DBP\",\"SBP\",\"MAP\"]].values\n","# cluster_data_3 = df_filled[[\"HR\",\"Resp\",\"O2Sat\",\"HospAdmTime\",\"Age\",\"Gender\"]].values\n","cluster_data_1 = df_filled[[\"Hour\",\"ICULOS\"]].values\n","cluster_data_2 = df_filled[[\"HospAdmTime\",\"DBP\",\"SBP\",\"MAP\"]].values\n","cluster_data_3 = df_filled[[\"HR\",\"Resp\",\"O2Sat\",\"Age\",\"Gender\"]].values\n","cluster_score_1 = pca.fit_transform(StandardScaler().fit_transform(cluster_data_1))                  \n","cluster_score_2 = pca.fit_transform(StandardScaler().fit_transform(cluster_data_2))                  \n","cluster_score_3 = pca.fit_transform(StandardScaler().fit_transform(cluster_data_3))                  \n","# (cluster_score_1+cluster_score_2+cluster_score_3)\n","x = np.array([cluster_score_1.flatten(),cluster_score_2.flatten(),cluster_score_3.flatten()])\n","X = x.T\n","y = sepsis.to_numpy()\n","print(X.shape)\n","print(y.shape)\n","print(type(y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"usK7gUK-csKF","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split\n","from sklearn.metrics import precision_score, recall_score, accuracy_score\n","from sklearn.tree import DecisionTreeClassifier\n","group = df_filled[\"Identifier\"].to_numpy()\n","train_pred = []\n","train_actual = []\n","\n","test_pred = []\n","test_actual = []\n","\n","kf = GroupKFold(n_splits=5)\n","for train_idx, test_idx in kf.split(X, y, group):\n","   X_train, y_train = X[train_idx, :], y[train_idx]\n","   X_test, y_test = X[test_idx, :], y[test_idx]\n","   \n","   # Decision tree classifier with higher penalty\n","   # for misclassifying the low frequency output\n","   # label\n","   clf = DecisionTreeClassifier(class_weight=\"balanced\",\n","                                max_depth=20,\n","                                max_leaf_nodes=20)\n","   model = clf.fit(X_train, y_train)\n","\n","   train_pred.extend(clf.predict(X_train))\n","   train_actual.extend(y_train)\n","\n","   test_pred.extend(clf.predict(X_test))\n","   test_actual.extend(y_test)\n","\n","\n","\n","# Function for evaluating train and test accuracy\n","def evaluate(actual, predicted, prefix=\"\"):\n","    precision = precision_score(actual, predicted)\n","    recall = recall_score(actual, predicted)\n","    accuracy = accuracy_score(actual, predicted)\n","\n","    print(\"%s Precision: %.3f%%, Recall: %.3f%%, Accuracy: %.3f%%\" % (prefix, precision * 100, recall * 100, accuracy * 100))\n","\n","\n","\n","# Evaluate train and test accuracy\n","evaluate(train_actual, train_pred, \"Train\")\n","evaluate(test_actual, test_pred, \"Test\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDUIQW2r-AuN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}